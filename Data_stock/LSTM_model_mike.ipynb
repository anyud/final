{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras_tuner as kt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Normalize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = 'D:\\\\Github anyud\\\\final\\\\Data_stock\\\\processed_stock_data.csv'\n",
    "merged_df = pd.read_csv(file_path)\n",
    "\n",
    "# Normalize data\n",
    "price_scaler = MinMaxScaler()\n",
    "merged_df[['Price']] = price_scaler.fit_transform(merged_df[['Price']])\n",
    "\n",
    "feature_scaler = MinMaxScaler()\n",
    "merged_df[['MA30', 'MA90']] = feature_scaler.fit_transform(merged_df[['MA30', 'MA90']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Sequences for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sequences for LSTM\n",
    "def create_sequences(df, time_steps=30):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(df) - time_steps):\n",
    "        sequence = df[['Price', 'MA30', 'MA90']].iloc[i:i+time_steps].values\n",
    "        label = df['Price'].iloc[i+time_steps]\n",
    "        sequences.append(sequence)\n",
    "        labels.append(label)\n",
    "    return np.array(sequences), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the LSTM HyperModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM HyperModel\n",
    "class LSTMHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        for i in range(hp.Int('num_layers', 1, 3)):\n",
    "            model.add(LSTM(units=hp.Int(f'units_{i}', min_value=32, max_value=512, step=32), \n",
    "                           return_sequences=(i != hp.Int('num_layers', 1, 3) - 1), input_shape=(time_steps, 3)))\n",
    "        model.add(Dense(1))  # Output layer should match the number of features\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(\n",
    "            hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG', default=1e-3)),\n",
    "            loss='mean_squared_error')\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate the Model for Each Ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_dir\\lstm_stock_model_AAA\\tuner0.json\n",
      "Ticker: AAA\n",
      "The hyperparameter search is complete. The optimal number of layers is 1.\n",
      "Layer 1: 224 units\n",
      "The optimal learning rate for the optimizer is 0.000699204056440982.\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - loss: 5.9492e-04 - val_loss: 3.6349e-05\n",
      "Epoch 2/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 3.1165e-05 - val_loss: 1.6093e-05\n",
      "Epoch 3/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 2.4071e-05 - val_loss: 1.0486e-05\n",
      "Epoch 4/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 1.6842e-05 - val_loss: 1.0739e-05\n",
      "Epoch 5/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 1.5713e-05 - val_loss: 6.6312e-06\n",
      "Epoch 6/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 1.5546e-05 - val_loss: 6.9151e-06\n",
      "Epoch 7/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 1.6617e-05 - val_loss: 6.3224e-06\n",
      "Epoch 8/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 62ms/step - loss: 1.3105e-05 - val_loss: 5.5572e-06\n",
      "Epoch 9/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - loss: 1.4068e-05 - val_loss: 5.3586e-06\n",
      "Epoch 10/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 55ms/step - loss: 1.3142e-05 - val_loss: 5.1077e-06\n",
      "Epoch 11/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 49ms/step - loss: 1.5448e-05 - val_loss: 5.0759e-06\n",
      "Epoch 12/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - loss: 1.1220e-05 - val_loss: 6.2755e-06\n",
      "Epoch 13/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - loss: 1.0377e-05 - val_loss: 4.6206e-06\n",
      "Epoch 14/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - loss: 1.0708e-05 - val_loss: 5.0425e-06\n",
      "Epoch 15/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - loss: 1.1618e-05 - val_loss: 8.5218e-06\n",
      "Epoch 16/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - loss: 1.0336e-05 - val_loss: 4.3535e-06\n",
      "Epoch 17/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - loss: 9.2319e-06 - val_loss: 4.0552e-06\n",
      "Epoch 18/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - loss: 9.5014e-06 - val_loss: 9.4999e-06\n",
      "Epoch 19/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - loss: 1.4660e-05 - val_loss: 4.9858e-06\n",
      "Epoch 20/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 60ms/step - loss: 9.9015e-06 - val_loss: 4.6796e-06\n",
      "Epoch 21/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - loss: 1.0565e-05 - val_loss: 4.0365e-06\n",
      "Epoch 22/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - loss: 8.8922e-06 - val_loss: 4.1744e-06\n",
      "Epoch 23/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - loss: 9.8845e-06 - val_loss: 3.8047e-06\n",
      "Epoch 24/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - loss: 8.3687e-06 - val_loss: 3.8299e-06\n",
      "Epoch 25/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - loss: 8.7112e-06 - val_loss: 4.4859e-06\n",
      "Epoch 26/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 1.0926e-05 - val_loss: 3.6062e-06\n",
      "Epoch 27/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: 8.1296e-06 - val_loss: 8.1089e-06\n",
      "Epoch 28/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: 9.3130e-06 - val_loss: 1.2549e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 1.2899e-05 - val_loss: 4.8480e-06\n",
      "Epoch 30/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 8.3132e-06 - val_loss: 6.8279e-06\n",
      "Epoch 31/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 9.2131e-06 - val_loss: 5.0859e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Reloading Tuner from my_dir\\lstm_stock_model_AAPL\\tuner0.json\n",
      "Ticker: AAPL\n",
      "The hyperparameter search is complete. The optimal number of layers is 1.\n",
      "Layer 1: 64 units\n",
      "The optimal learning rate for the optimizer is 0.0010817749459071228.\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 1.8538e-05 - val_loss: 3.8952e-07\n",
      "Epoch 2/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1.0997e-07 - val_loss: 2.1915e-07\n",
      "Epoch 3/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 6.0347e-08 - val_loss: 1.9225e-07\n",
      "Epoch 4/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 4.9296e-08 - val_loss: 1.8777e-07\n",
      "Epoch 5/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 4.4372e-08 - val_loss: 1.5301e-07\n",
      "Epoch 6/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 3.5528e-08 - val_loss: 2.6643e-07\n",
      "Epoch 7/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 3.2581e-08 - val_loss: 1.0000e-07\n",
      "Epoch 8/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 2.1950e-08 - val_loss: 6.6338e-08\n",
      "Epoch 9/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 1.7311e-08 - val_loss: 1.1210e-07\n",
      "Epoch 10/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 2.1500e-08 - val_loss: 3.6755e-08\n",
      "Epoch 11/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1.1147e-08 - val_loss: 2.0643e-08\n",
      "Epoch 12/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 7.5019e-09 - val_loss: 2.4210e-08\n",
      "Epoch 13/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 4.7814e-09 - val_loss: 2.0796e-08\n",
      "Epoch 14/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 3.8005e-09 - val_loss: 8.3667e-09\n",
      "Epoch 15/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 2.8016e-09 - val_loss: 6.5241e-09\n",
      "Epoch 16/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 2.2725e-09 - val_loss: 4.6613e-09\n",
      "Epoch 17/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 1.7172e-09 - val_loss: 5.7854e-09\n",
      "Epoch 18/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 1.6296e-09 - val_loss: 2.4631e-09\n",
      "Epoch 19/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 1.6391e-09 - val_loss: 2.1136e-09\n",
      "Epoch 20/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1.2040e-09 - val_loss: 2.2579e-09\n",
      "Epoch 21/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1.6239e-09 - val_loss: 1.9624e-09\n",
      "Epoch 22/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 9.4997e-10 - val_loss: 1.9906e-09\n",
      "Epoch 23/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1.1823e-09 - val_loss: 2.2190e-09\n",
      "Epoch 24/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1.3988e-09 - val_loss: 2.0081e-09\n",
      "Epoch 25/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1.4500e-09 - val_loss: 2.4311e-09\n",
      "Epoch 26/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1.0854e-09 - val_loss: 1.9885e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "Reloading Tuner from my_dir\\lstm_stock_model_ACB\\tuner0.json\n",
      "Ticker: ACB\n",
      "The hyperparameter search is complete. The optimal number of layers is 1.\n",
      "Layer 1: 288 units\n",
      "The optimal learning rate for the optimizer is 0.001945510412913914.\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 51ms/step - loss: 6.2973e-04 - val_loss: 3.3165e-05\n",
      "Epoch 2/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - loss: 2.3628e-05 - val_loss: 2.0039e-05\n",
      "Epoch 3/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - loss: 1.8964e-05 - val_loss: 1.7029e-05\n",
      "Epoch 4/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 49ms/step - loss: 1.3691e-05 - val_loss: 1.6247e-05\n",
      "Epoch 5/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - loss: 1.2981e-05 - val_loss: 3.2031e-05\n",
      "Epoch 6/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - loss: 1.3931e-05 - val_loss: 2.7599e-05\n",
      "Epoch 7/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - loss: 1.2244e-05 - val_loss: 2.5245e-05\n",
      "Epoch 8/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 49ms/step - loss: 1.0967e-05 - val_loss: 1.8312e-05\n",
      "Epoch 9/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - loss: 1.0775e-05 - val_loss: 1.4720e-05\n",
      "Epoch 10/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - loss: 1.1328e-05 - val_loss: 1.4453e-05\n",
      "Epoch 11/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - loss: 1.5555e-05 - val_loss: 1.5069e-05\n",
      "Epoch 12/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - loss: 1.0043e-05 - val_loss: 1.5926e-05\n",
      "Epoch 13/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - loss: 7.6983e-06 - val_loss: 2.1972e-05\n",
      "Epoch 14/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - loss: 7.6020e-06 - val_loss: 2.2821e-05\n",
      "Epoch 15/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - loss: 9.3846e-06 - val_loss: 1.4518e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "Reloading Tuner from my_dir\\lstm_stock_model_BID\\tuner0.json\n",
      "Ticker: BID\n",
      "The hyperparameter search is complete. The optimal number of layers is 3.\n",
      "Layer 1: 320 units\n",
      "Layer 2: 96 units\n",
      "Layer 3: 192 units\n",
      "The optimal learning rate for the optimizer is 0.0023311662519474776.\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 111ms/step - loss: 0.0047 - val_loss: 1.1429e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - loss: 1.1942e-04 - val_loss: 7.6238e-05\n",
      "Epoch 3/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 1.2335e-04 - val_loss: 1.1140e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 7.5028e-05 - val_loss: 7.7457e-05\n",
      "Epoch 5/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 7.8460e-05 - val_loss: 2.1053e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 6.1580e-05 - val_loss: 9.3644e-05\n",
      "Epoch 7/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 6.2079e-05 - val_loss: 5.3298e-05\n",
      "Epoch 8/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 6.4365e-05 - val_loss: 1.2581e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 6.1244e-05 - val_loss: 7.8296e-05\n",
      "Epoch 10/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 5.9334e-05 - val_loss: 4.9218e-05\n",
      "Epoch 11/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 4.9061e-05 - val_loss: 1.0352e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 6.1577e-05 - val_loss: 4.5794e-05\n",
      "Epoch 13/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - loss: 6.3909e-05 - val_loss: 4.1108e-05\n",
      "Epoch 14/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 4.0854e-05 - val_loss: 5.4581e-05\n",
      "Epoch 15/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 3.8497e-05 - val_loss: 9.6613e-05\n",
      "Epoch 16/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - loss: 3.9562e-05 - val_loss: 7.1255e-05\n",
      "Epoch 17/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - loss: 5.0374e-05 - val_loss: 3.6331e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 3.7847e-05 - val_loss: 8.7433e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 3.6675e-05 - val_loss: 3.5643e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 2.9985e-05 - val_loss: 3.8398e-05\n",
      "Epoch 21/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 3.3907e-05 - val_loss: 9.7875e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 3.1943e-05 - val_loss: 7.2921e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 3.0425e-05 - val_loss: 5.3110e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 3.5012e-05 - val_loss: 6.1552e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "Reloading Tuner from my_dir\\lstm_stock_model_CTG\\tuner0.json\n",
      "Ticker: CTG\n",
      "The hyperparameter search is complete. The optimal number of layers is 3.\n",
      "Layer 1: 384 units\n",
      "Layer 2: 352 units\n",
      "Layer 3: 32 units\n",
      "The optimal learning rate for the optimizer is 0.0008569717606115053.\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 64ms/step - loss: 0.0017 - val_loss: 9.4055e-05\n",
      "Epoch 2/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - loss: 8.5115e-05 - val_loss: 6.1013e-05\n",
      "Epoch 3/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - loss: 5.7390e-05 - val_loss: 7.4032e-05\n",
      "Epoch 4/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - loss: 7.1131e-05 - val_loss: 9.1872e-05\n",
      "Epoch 5/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - loss: 4.9257e-05 - val_loss: 5.7317e-05\n",
      "Epoch 6/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - loss: 5.2498e-05 - val_loss: 3.3304e-05\n",
      "Epoch 7/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - loss: 4.2499e-05 - val_loss: 8.8374e-05\n",
      "Epoch 8/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 63ms/step - loss: 5.3393e-05 - val_loss: 4.4258e-05\n",
      "Epoch 9/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 106ms/step - loss: 4.4292e-05 - val_loss: 3.3658e-05\n",
      "Epoch 10/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 131ms/step - loss: 3.9360e-05 - val_loss: 2.9202e-05\n",
      "Epoch 11/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 126ms/step - loss: 3.6828e-05 - val_loss: 4.4345e-05\n",
      "Epoch 12/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 119ms/step - loss: 3.6986e-05 - val_loss: 5.1482e-05\n",
      "Epoch 13/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 116ms/step - loss: 3.0502e-05 - val_loss: 5.0989e-05\n",
      "Epoch 14/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 125ms/step - loss: 3.8788e-05 - val_loss: 6.2316e-05\n",
      "Epoch 15/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 130ms/step - loss: 3.8053e-05 - val_loss: 6.8982e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step\n",
      "Reloading Tuner from my_dir\\lstm_stock_model_FPT\\tuner0.json\n",
      "Ticker: FPT\n",
      "The hyperparameter search is complete. The optimal number of layers is 3.\n",
      "Layer 1: 160 units\n",
      "Layer 2: 256 units\n",
      "Layer 3: 96 units\n",
      "The optimal learning rate for the optimizer is 0.0012564412521369585.\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 95ms/step - loss: 0.0051 - val_loss: 0.0019\n",
      "Epoch 2/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 85ms/step - loss: 3.2414e-04 - val_loss: 0.0027\n",
      "Epoch 3/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 87ms/step - loss: 2.5606e-04 - val_loss: 0.0011\n",
      "Epoch 4/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 83ms/step - loss: 1.8933e-04 - val_loss: 0.0012\n",
      "Epoch 5/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 85ms/step - loss: 1.6350e-04 - val_loss: 0.0013\n",
      "Epoch 6/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 85ms/step - loss: 1.3016e-04 - val_loss: 3.7718e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 79ms/step - loss: 9.8044e-05 - val_loss: 8.4953e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 83ms/step - loss: 9.1449e-05 - val_loss: 3.1723e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 84ms/step - loss: 6.7717e-05 - val_loss: 2.5518e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 85ms/step - loss: 1.1047e-04 - val_loss: 0.0011\n",
      "Epoch 11/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 85ms/step - loss: 1.1516e-04 - val_loss: 3.4676e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 85ms/step - loss: 1.0400e-04 - val_loss: 2.6170e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 81ms/step - loss: 5.4705e-05 - val_loss: 2.8784e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 81ms/step - loss: 7.7306e-05 - val_loss: 3.4690e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step\n",
      "Reloading Tuner from my_dir\\lstm_stock_model_GAS\\tuner0.json\n",
      "Ticker: GAS\n",
      "The hyperparameter search is complete. The optimal number of layers is 1.\n",
      "Layer 1: 288 units\n",
      "The optimal learning rate for the optimizer is 0.003907263893779795.\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - loss: 5.3662e-04 - val_loss: 1.3211e-08\n",
      "Epoch 2/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - loss: 2.1943e-07 - val_loss: 3.7250e-07\n",
      "Epoch 3/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - loss: 8.5855e-08 - val_loss: 2.6750e-07\n",
      "Epoch 4/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - loss: 7.7609e-08 - val_loss: 3.2278e-07\n",
      "Epoch 5/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 7.1004e-08 - val_loss: 2.6480e-07\n",
      "Epoch 6/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - loss: 7.0329e-08 - val_loss: 2.3213e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "Reloading Tuner from my_dir\\lstm_stock_model_NVDA\\tuner0.json\n",
      "Ticker: NVDA\n",
      "The hyperparameter search is complete. The optimal number of layers is 1.\n",
      "Layer 1: 512 units\n",
      "The optimal learning rate for the optimizer is 0.0005794155666227713.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 94ms/step - loss: 3.5482e-05 - val_loss: 5.5737e-06\n",
      "Epoch 2/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 95ms/step - loss: 2.1404e-07 - val_loss: 5.2733e-06\n",
      "Epoch 3/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - loss: 1.5102e-07 - val_loss: 4.6023e-06\n",
      "Epoch 4/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 85ms/step - loss: 1.2923e-07 - val_loss: 3.7389e-06\n",
      "Epoch 5/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 100ms/step - loss: 1.1236e-07 - val_loss: 3.0760e-06\n",
      "Epoch 6/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 100ms/step - loss: 9.4836e-08 - val_loss: 2.9226e-06\n",
      "Epoch 7/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 100ms/step - loss: 6.6279e-08 - val_loss: 2.0214e-06\n",
      "Epoch 8/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 99ms/step - loss: 5.7294e-08 - val_loss: 1.7925e-06\n",
      "Epoch 9/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 89ms/step - loss: 3.5341e-08 - val_loss: 1.2092e-06\n",
      "Epoch 10/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 85ms/step - loss: 2.9142e-08 - val_loss: 9.8221e-07\n",
      "Epoch 11/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 100ms/step - loss: 1.9336e-08 - val_loss: 6.3573e-07\n",
      "Epoch 12/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 87ms/step - loss: 1.4826e-08 - val_loss: 5.5640e-07\n",
      "Epoch 13/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 96ms/step - loss: 1.1711e-08 - val_loss: 3.8833e-07\n",
      "Epoch 14/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 102ms/step - loss: 8.0955e-09 - val_loss: 2.7321e-07\n",
      "Epoch 15/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 101ms/step - loss: 7.4765e-09 - val_loss: 2.6972e-07\n",
      "Epoch 16/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 103ms/step - loss: 7.2928e-09 - val_loss: 2.5382e-07\n",
      "Epoch 17/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - loss: 7.0306e-09 - val_loss: 1.5472e-07\n",
      "Epoch 18/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 81ms/step - loss: 6.2610e-09 - val_loss: 1.3921e-07\n",
      "Epoch 19/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 81ms/step - loss: 7.7760e-09 - val_loss: 1.6031e-07\n",
      "Epoch 20/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 100ms/step - loss: 5.7166e-09 - val_loss: 1.6352e-07\n",
      "Epoch 21/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 89ms/step - loss: 6.5909e-09 - val_loss: 2.4272e-07\n",
      "Epoch 22/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 73ms/step - loss: 1.1228e-08 - val_loss: 2.0587e-07\n",
      "Epoch 23/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 104ms/step - loss: 6.4604e-09 - val_loss: 1.8952e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step\n",
      "Reloading Tuner from my_dir\\lstm_stock_model_VCB\\tuner0.json\n",
      "Ticker: VCB\n",
      "The hyperparameter search is complete. The optimal number of layers is 2.\n",
      "Layer 1: 160 units\n",
      "Layer 2: 320 units\n",
      "The optimal learning rate for the optimizer is 0.0011109201683730445.\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 77ms/step - loss: 0.0096 - val_loss: 3.7480e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 79ms/step - loss: 2.7652e-04 - val_loss: 3.6426e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 80ms/step - loss: 1.8191e-04 - val_loss: 2.9954e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 78ms/step - loss: 1.7183e-04 - val_loss: 4.3848e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - loss: 1.5588e-04 - val_loss: 1.4570e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 78ms/step - loss: 1.3965e-04 - val_loss: 2.0712e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 78ms/step - loss: 1.3949e-04 - val_loss: 1.2399e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 83ms/step - loss: 1.3624e-04 - val_loss: 1.1857e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 81ms/step - loss: 1.2912e-04 - val_loss: 2.3915e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 82ms/step - loss: 1.2859e-04 - val_loss: 2.0146e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 82ms/step - loss: 1.5321e-04 - val_loss: 1.5061e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 82ms/step - loss: 1.0590e-04 - val_loss: 5.1374e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 81ms/step - loss: 1.2746e-04 - val_loss: 1.1410e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 84ms/step - loss: 1.2201e-04 - val_loss: 1.0786e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - loss: 1.0090e-04 - val_loss: 1.2046e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 79ms/step - loss: 8.6946e-05 - val_loss: 1.4128e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 83ms/step - loss: 1.0653e-04 - val_loss: 1.0175e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 83ms/step - loss: 8.7063e-05 - val_loss: 9.1024e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 83ms/step - loss: 8.7393e-05 - val_loss: 1.1996e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 79ms/step - loss: 8.5721e-05 - val_loss: 2.6957e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 79ms/step - loss: 1.0899e-04 - val_loss: 3.3471e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 81ms/step - loss: 1.0408e-04 - val_loss: 8.9780e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 82ms/step - loss: 6.8106e-05 - val_loss: 9.4801e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 80ms/step - loss: 6.6070e-05 - val_loss: 1.1541e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 79ms/step - loss: 1.0698e-04 - val_loss: 1.6112e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 82ms/step - loss: 8.7061e-05 - val_loss: 7.1913e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 78ms/step - loss: 6.9126e-05 - val_loss: 2.4435e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 78ms/step - loss: 7.1436e-05 - val_loss: 8.6810e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 81ms/step - loss: 5.9537e-05 - val_loss: 6.8616e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - loss: 7.4608e-05 - val_loss: 1.0434e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 80ms/step - loss: 6.6897e-05 - val_loss: 2.1436e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 78ms/step - loss: 8.1885e-05 - val_loss: 7.9964e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 77ms/step - loss: 5.9541e-05 - val_loss: 8.8152e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 80ms/step - loss: 9.5296e-05 - val_loss: 1.6812e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step\n",
      "Reloading Tuner from my_dir\\lstm_stock_model_VNM\\tuner0.json\n",
      "Ticker: VNM\n",
      "The hyperparameter search is complete. The optimal number of layers is 2.\n",
      "Layer 1: 256 units\n",
      "Layer 2: 384 units\n",
      "The optimal learning rate for the optimizer is 0.001266095545138909.\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 126ms/step - loss: 0.0428 - val_loss: 6.5142e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 113ms/step - loss: 0.0010 - val_loss: 1.4314e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 112ms/step - loss: 6.3521e-04 - val_loss: 1.2669e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 105ms/step - loss: 5.4593e-04 - val_loss: 2.7484e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 107ms/step - loss: 6.4204e-04 - val_loss: 5.1607e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 109ms/step - loss: 6.4827e-04 - val_loss: 1.5632e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 104ms/step - loss: 3.5248e-04 - val_loss: 2.7443e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 114ms/step - loss: 7.1000e-04 - val_loss: 2.1650e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model for each ticker\n",
    "tickers = merged_df['Stock_Name'].unique()\n",
    "\n",
    "for ticker in tickers:\n",
    "    ticker_df = merged_df[merged_df['Stock_Name'] == ticker].dropna()\n",
    "    \n",
    "    # Create sequences and labels\n",
    "    time_steps = 30\n",
    "    X, y = create_sequences(ticker_df, time_steps)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "    # Initialize RandomSearch Tuner\n",
    "    tuner = kt.RandomSearch(\n",
    "        LSTMHyperModel(),\n",
    "        objective='val_loss',\n",
    "        max_trials=20,\n",
    "        executions_per_trial=1,\n",
    "        directory='my_dir',\n",
    "        project_name=f'lstm_stock_model_{ticker}'\n",
    "    )\n",
    "\n",
    "    # Perform hyperparameter search\n",
    "    tuner.search(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=1, callbacks=[EarlyStopping(monitor='val_loss', patience=5)])\n",
    "\n",
    "    # Retrieve the best hyperparameters\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    print(f\"Ticker: {ticker}\")\n",
    "    print(f\"The hyperparameter search is complete. The optimal number of layers is {best_hps.get('num_layers')}.\")\n",
    "    for i in range(best_hps.get('num_layers')):\n",
    "        print(f\"Layer {i + 1}: {best_hps.get(f'units_{i}')} units\")\n",
    "    print(f\"The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\")\n",
    "\n",
    "    # Define additional hyperparameters or settings\n",
    "    num_epochs = 50\n",
    "    batch_size = 32\n",
    "\n",
    "    # Create directory for ticker if it doesn't exist\n",
    "    os.makedirs(f'models/{ticker}', exist_ok=True)\n",
    "\n",
    "    # Save hyperparameters to a text file\n",
    "    with open(f'models/{ticker}/best_hyperparameters_{ticker}.txt', 'w') as f:\n",
    "        f.write(f\"Ticker: {ticker}\\n\")\n",
    "        f.write(f\"The hyperparameter search is complete. The optimal number of layers is {best_hps.get('num_layers')}.\\n\")\n",
    "        for i in range(best_hps.get('num_layers')):\n",
    "            f.write(f\"Layer {i + 1}: {best_hps.get(f'units_{i}')} units\\n\")\n",
    "        f.write(f\"The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\\n\")\n",
    "        f.write(f\"Epochs: {num_epochs}\\n\")\n",
    "        f.write(f\"Batch size: {batch_size}\\n\")\n",
    "\n",
    "    # Build the model with the optimal hyperparameters\n",
    "    model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=1, callbacks=[EarlyStopping(monitor='val_loss', patience=5)])\n",
    "\n",
    "    # Save the model\n",
    "    model.save(f'models/{ticker}/lstm_stock_model_best_{ticker}.h5')\n",
    "\n",
    "    # Save the scalers\n",
    "    joblib.dump(price_scaler, f'models/{ticker}/{ticker}_price_scaler.pkl')\n",
    "    joblib.dump(feature_scaler, f'models/{ticker}/{ticker}_feature_scaler.pkl')\n",
    "\n",
    "    # Save the history for plotting\n",
    "    with open(f'models/{ticker}/history_{ticker}.pkl', 'wb') as file:\n",
    "        joblib.dump(history.history, file)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Inverse transform the predictions and the actual values\n",
    "    y_pred = price_scaler.inverse_transform(y_pred)\n",
    "    y_test = price_scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "    # Save the predictions for plotting\n",
    "    np.save(f'models/{ticker}/y_test_{ticker}.npy', y_test)\n",
    "    np.save(f'models/{ticker}/y_pred_{ticker}.npy', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training and validation loss\n",
    "def plot_training_validation_loss(ticker):\n",
    "    with open(f'models/{ticker}/history_{ticker}.pkl', 'rb') as file:\n",
    "        history = joblib.load(file)\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(history['loss'], label='Training Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'Training and Validation Loss for {ticker}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'models/{ticker}/training_validation_loss_{ticker}.png')  # Save the plot\n",
    "    plt.close()\n",
    "\n",
    "# Plotting the stock price prediction\n",
    "def plot_stock_price_prediction(ticker):\n",
    "    y_test = np.load(f'models/{ticker}/y_test_{ticker}.npy')\n",
    "    y_pred = np.load(f'models/{ticker}/y_pred_{ticker}.npy')\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(y_test, color='blue', label='Actual Stock Price')\n",
    "    plt.plot(y_pred, color='red', label='Predicted Stock Price')\n",
    "    plt.title(f'Stock Price Prediction for {ticker}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Stock Price')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'models/{ticker}/stock_price_prediction_{ticker}.png')  # Save the plot\n",
    "    plt.close()\n",
    "\n",
    "# Plot results for each ticker\n",
    "for ticker in tickers:\n",
    "    plot_training_validation_loss(ticker)\n",
    "    plot_stock_price_prediction(ticker)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock: VNM\n",
      "The hyperparameter search is complete. The optimal number of layers is 1.\n",
      "Layer 1: 288 units\n",
      "The optimal learning rate for the optimizer is 0.001945510412913914.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(f'my_dir/lstm_stock_model_{ticker}', exist_ok=True)\n",
    "\n",
    "# Print and save hyperparameters\n",
    "with open(f'my_dir/lstm_stock_model_{ticker}/best_hyperparameters_{ticker}.txt', 'w') as f:\n",
    "    f.write(f\"Stock: {ticker}\\n\")\n",
    "    f.write(f\"The hyperparameter search is complete. The optimal number of layers is {best_hps.get('num_layers')}.\\n\")\n",
    "    for i in range(best_hps.get('num_layers')):\n",
    "        f.write(f\"Layer {i + 1}: {best_hps.get(f'units_{i}')} units\\n\")\n",
    "    f.write(f\"The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\\n\")\n",
    "\n",
    "print(f\"Stock: {ticker}\")\n",
    "print(f\"The hyperparameter search is complete. The optimal number of layers is {best_hps.get('num_layers')}.\")\n",
    "for i in range(best_hps.get('num_layers')):\n",
    "    print(f\"Layer {i + 1}: {best_hps.get(f'units_{i}')} units\")\n",
    "print(f\"The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for AAA:\n",
      "Mean Squared Error (MSE): 156778.47056998318\n",
      "Root Mean Squared Error (RMSE): 395.9526115206\n",
      "R-squared (R2): 0.9310251627274403\n",
      "Mean Absolute Error (MAE): 329.5222014696609\n",
      "Accuracy (within 5% tolerance): 79.5964125560538%\n",
      "--------------------------\n",
      "\n",
      "Metrics for AAPL:\n",
      "Mean Squared Error (MSE): 61.29782492088633\n",
      "Root Mean Squared Error (RMSE): 7.829292747169844\n",
      "R-squared (R2): 0.8196670829334662\n",
      "Mean Absolute Error (MAE): 6.852946345631668\n",
      "Accuracy (within 5% tolerance): 65.18847006651885%\n",
      "--------------------------\n",
      "\n",
      "Metrics for ACB:\n",
      "Mean Squared Error (MSE): 447523.6327764048\n",
      "Root Mean Squared Error (RMSE): 668.9720717462013\n",
      "R-squared (R2): 0.9543389790711115\n",
      "Mean Absolute Error (MAE): 507.6501217418723\n",
      "Accuracy (within 5% tolerance): 91.03139013452915%\n",
      "--------------------------\n",
      "\n",
      "Metrics for BID:\n",
      "Mean Squared Error (MSE): 1897392.3296908601\n",
      "Root Mean Squared Error (RMSE): 1377.4586489949018\n",
      "R-squared (R2): 0.9528539294064157\n",
      "Mean Absolute Error (MAE): 1110.4909859130314\n",
      "Accuracy (within 5% tolerance): 88.59060402684564%\n",
      "--------------------------\n",
      "\n",
      "Metrics for CTG:\n",
      "Mean Squared Error (MSE): 2126436.842620807\n",
      "Root Mean Squared Error (RMSE): 1458.2307233839256\n",
      "R-squared (R2): 0.8649452196374787\n",
      "Mean Absolute Error (MAE): 1249.0824673168343\n",
      "Accuracy (within 5% tolerance): 62.41610738255034%\n",
      "--------------------------\n",
      "\n",
      "Metrics for FPT:\n",
      "Mean Squared Error (MSE): 10693517.19257319\n",
      "Root Mean Squared Error (RMSE): 3270.094370591343\n",
      "R-squared (R2): 0.9740738880504881\n",
      "Mean Absolute Error (MAE): 2367.41373916387\n",
      "Accuracy (within 5% tolerance): 87.02460850111858%\n",
      "--------------------------\n",
      "\n",
      "Metrics for GAS:\n",
      "Mean Squared Error (MSE): 7155.550277592381\n",
      "Root Mean Squared Error (RMSE): 84.59048573919162\n",
      "R-squared (R2): -20.051012110787006\n",
      "Mean Absolute Error (MAE): 82.86551129190991\n",
      "Accuracy (within 5% tolerance): 0.0%\n",
      "--------------------------\n",
      "\n",
      "Metrics for NVDA:\n",
      "Mean Squared Error (MSE): 5842.0973494098525\n",
      "Root Mean Squared Error (RMSE): 76.43361400202042\n",
      "R-squared (R2): 0.9142352110673352\n",
      "Mean Absolute Error (MAE): 55.34649423332806\n",
      "Accuracy (within 5% tolerance): 17.29490022172949%\n",
      "--------------------------\n",
      "\n",
      "Metrics for VCB:\n",
      "Mean Squared Error (MSE): 5182461.671000129\n",
      "Root Mean Squared Error (RMSE): 2276.5020691842406\n",
      "R-squared (R2): 0.9529011048423195\n",
      "Mean Absolute Error (MAE): 1953.5790076202463\n",
      "Accuracy (within 5% tolerance): 95.07829977628636%\n",
      "--------------------------\n",
      "\n",
      "Metrics for VNM:\n",
      "Mean Squared Error (MSE): 6673787.850658118\n",
      "Root Mean Squared Error (RMSE): 2583.367540761112\n",
      "R-squared (R2): 0.5244673558419295\n",
      "Mean Absolute Error (MAE): 2153.8500157298663\n",
      "Accuracy (within 5% tolerance): 83.22147651006712%\n",
      "--------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "# Function to calculate and save metrics\n",
    "def calculate_metrics(y_true, y_pred, ticker):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    \n",
    "    accuracy = np.mean(np.abs((y_true - y_pred) / y_true) < 0.05) * 100  # Within 5% tolerance\n",
    "    \n",
    "    metrics = (\n",
    "        f\"Metrics for {ticker}:\\n\"\n",
    "        f\"Mean Squared Error (MSE): {mse}\\n\"\n",
    "        f\"Root Mean Squared Error (RMSE): {rmse}\\n\"\n",
    "        f\"R-squared (R2): {r2}\\n\"\n",
    "        f\"Mean Absolute Error (MAE): {mae}\\n\"\n",
    "        f\"Accuracy (within 5% tolerance): {accuracy}%\\n\"\n",
    "        \"--------------------------\\n\"\n",
    "    )\n",
    "    \n",
    "    # Print metrics to console\n",
    "    print(metrics)\n",
    "    \n",
    "    # Save metrics to a file\n",
    "    with open(f'models/{ticker}/metrics_{ticker}.txt', 'w') as file:\n",
    "        file.write(metrics)\n",
    "\n",
    "# Plotting the training and validation loss\n",
    "def plot_training_validation_loss(ticker):\n",
    "    with open(f'models/{ticker}/history_{ticker}.pkl', 'rb') as file:\n",
    "        history = joblib.load(file)\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(history['loss'], label='Training Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'Training and Validation Loss for {ticker}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'models/{ticker}/training_validation_loss_{ticker}.png')  # Save the plot\n",
    "    plt.close()\n",
    "\n",
    "# Plotting the stock price prediction\n",
    "def plot_stock_price_prediction(ticker):\n",
    "    y_test = np.load(f'models/{ticker}/y_test_{ticker}.npy')\n",
    "    y_pred = np.load(f'models/{ticker}/y_pred_{ticker}.npy')\n",
    "\n",
    "    # Calculate and save metrics\n",
    "    calculate_metrics(y_test, y_pred, ticker)\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(y_test, color='blue', label='Actual Stock Price')\n",
    "    plt.plot(y_pred, color='red', label='Predicted Stock Price')\n",
    "    plt.title(f'Stock Price Prediction for {ticker}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Stock Price')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'models/{ticker}/stock_price_prediction_{ticker}.png')  # Save the plot\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Plot results for each ticker\n",
    "for ticker in tickers:\n",
    "    plot_training_validation_loss(ticker)\n",
    "    plot_stock_price_prediction(ticker)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset\n",
    "file_path = 'D:\\\\Github Mikezxc\\\\Big-data-stock-real-time-platform\\\\merged_data_with_ma.csv'\n",
    "merged_df = pd.read_csv(file_path)\n",
    "\n",
    "# Normalize data\n",
    "price_scaler = MinMaxScaler()\n",
    "merged_df[['close']] = price_scaler.fit_transform(merged_df[['close']])\n",
    "\n",
    "feature_scaler = MinMaxScaler()\n",
    "merged_df[['MA30', 'MA90']] = feature_scaler.fit_transform(merged_df[['MA30', 'MA90']])\n",
    "\n",
    "# Prepare sequences for LSTM\n",
    "def create_sequences(df, time_steps=30):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(df) - time_steps):\n",
    "        sequence = df[['close', 'MA30', 'MA90']].iloc[i:i+time_steps].values\n",
    "        label = df['close'].iloc[i+time_steps]\n",
    "        sequences.append(sequence)\n",
    "        labels.append(label)\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "# Define the LSTM HyperModel\n",
    "class LSTMHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        for i in range(hp.Int('num_layers', 1, 3)):\n",
    "            model.add(LSTM(units=hp.Int(f'units_{i}', min_value=32, max_value=512, step=32), \n",
    "                           return_sequences=(i != hp.Int('num_layers', 1, 3) - 1), input_shape=(time_steps, 3)))\n",
    "        model.add(Dense(1))  # Output layer should match the number of features\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(\n",
    "            hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG', default=1e-3)),\n",
    "            loss='mean_squared_error')\n",
    "        return model\n",
    "\n",
    "# Train and evaluate the model for each ticker\n",
    "tickers = merged_df['ticker'].unique()\n",
    "\n",
    "for ticker in tickers:\n",
    "    ticker_df = merged_df[merged_df['ticker'] == ticker].dropna()\n",
    "    \n",
    "    # Create sequences and labels\n",
    "    time_steps = 30\n",
    "    X, y = create_sequences(ticker_df, time_steps)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "    # Initialize RandomSearch Tuner\n",
    "    tuner = kt.RandomSearch(\n",
    "        LSTMHyperModel(),\n",
    "        objective='val_loss',\n",
    "        max_trials=20,\n",
    "        executions_per_trial=1,\n",
    "        directory='my_dir',\n",
    "        project_name=f'lstm_stock_model_{ticker}'\n",
    "    )\n",
    "\n",
    "    # Perform hyperparameter search\n",
    "    tuner.search(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=1, callbacks=[EarlyStopping(monitor='val_loss', patience=5)])\n",
    "\n",
    "    # Retrieve the best hyperparameters\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    print(f\"Ticker: {ticker}\")\n",
    "    print(f\"The hyperparameter search is complete. The optimal number of layers is {best_hps.get('num_layers')}.\")\n",
    "    for i in range(best_hps.get('num_layers')):\n",
    "        print(f\"Layer {i + 1}: {best_hps.get(f'units_{i}')} units\")\n",
    "    print(f\"The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\")\n",
    "\n",
    "    # Build the model with the optimal hyperparameters\n",
    "    model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=1, callbacks=[EarlyStopping(monitor='val_loss', patience=5)])\n",
    "\n",
    "    # Save the model\n",
    "    model.save(f'/mnt/data/lstm_stock_model_best_{ticker}.h5')\n",
    "\n",
    "    # Save the scalers\n",
    "    joblib.dump(price_scaler, f'/mnt/data/{ticker}_price_scaler.pkl')\n",
    "    joblib.dump(feature_scaler, f'/mnt/data/{ticker}_feature_scaler.pkl')\n",
    "\n",
    "    # Plot the training and validation loss\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'Training and Validation Loss for {ticker}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Inverse transform the predictions and the actual values\n",
    "    y_pred = price_scaler.inverse_transform(y_pred)\n",
    "    y_test = price_scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(y_test, color='blue', label='Actual Stock Price')\n",
    "    plt.plot(y_pred, color='red', label='Predicted Stock Price')\n",
    "    plt.title(f'Stock Price Prediction for {ticker}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Stock Price')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
